# backend/models/rag/generate_question.py

from typing import List, Tuple
from langchain.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEndpoint
from backend.models.rag.query_rag import answer_question
from openai import AzureOpenAI
import streamlit as st

AZURE_API_KEY = st.secrets["AZURE_API_KEY"]
AZURE_ENDPOINT = st.secrets["AZURE_ENDPOINT"]
AZURE_DEPLOYMENT = "gpt-4o"  # ou ton nom de d√©ploiement
AZURE_API_VERSION = st.secrets["AZURE_API_VERSION"]



client = AzureOpenAI(
    api_version=AZURE_API_VERSION,
    azure_endpoint=AZURE_ENDPOINT,
    api_key=AZURE_API_KEY
)


# === Prompt Template pour g√©n√©rer la prochaine question
prompt_template = PromptTemplate.from_template("""
Tu es un conseiller en orientation scolaire et professionnelle.
Tu dois poser une s√©rie de 7 questions personnalis√©es pour aider un jeune √† d√©couvrir les secteurs d'activit√©s et les m√©tiers qui pourraient lui correspondre.

Voici l'historique des questions d√©j√† pos√©es et des r√©ponses fournies :
{history}

Propose maintenant une nouvelle question simple, claire et ferm√©e (avec des choix). Ne pose pas une question d√©j√† pos√©e.
Ta r√©ponse doit √™tre au format :
Question: <texte de la question>
Choix: <choix1>, <choix2>, <choix3>, <choix4>
""")


def generate_next_question(history_pairs: list[tuple[str, str]]) -> str:
    # Reformatage de l'historique
    history_text = "\n".join([f"Q: {q}\nR: {r}" for q, r in history_pairs]) or "(aucune question pos√©e)"

    prompt = f"""
Tu joues le r√¥le d‚Äôun conseiller d‚Äôorientation expert. Tu vas poser 7 questions √† un utilisateur pour cerner son profil.

Voici les √©changes pr√©c√©dents :
{history_text}

Pose-moi une nouvelle question (courte, claire, adapt√©e √† un jeune) pour continuer √† mieux cerner ma personnalit√© et mes pr√©f√©rences professionnelles.

R√©ponds uniquement par la question, sans explication ni commentaire.
""".strip()

    # Appel au LLM
    response = answer_question(prompt)

    # Debug
    print("üß† R√©ponse du LLM pour la question suivante :")
    print(response)

    # Nettoyage de la r√©ponse : on garde la premi√®re ligne non vide
    lines = [line.strip() for line in response.split("\n") if line.strip()]
    if not lines:
        raise ValueError("Format de la r√©ponse du LLM incorrect.")
    
    return response

def generate_final_recommendation(history_pairs: List[Tuple[str, str]]) -> str:
    history_text = "\n".join([f"Q: {q}\nR: {r}" for q, r in history_pairs])
    prompt = f"""
Tu es un conseiller d‚Äôorientation scolaire et professionnelle. Voici le profil d‚Äôun utilisateur bas√© sur ses r√©ponses √† 7 questions :

{history_text}

√Ä partir de ces r√©ponses, propose 3 secteurs d‚Äôactivit√© dans lesquels il pourrait s‚Äô√©panouir.
Pour chaque secteur, donne 2 m√©tiers pertinents (un tr√®s connu, un moins connu).
Explique bri√®vement pourquoi ces suggestions sont adapt√©es √† la personne.
Donne une r√©ponse claire, bienveillante, professionnelle et directement exploitable.
"""

    response = client.chat.completions.create(
        model=AZURE_DEPLOYMENT,
        messages=[
            {"role": "system", "content": "Tu es un assistant d‚Äôorientation scolaire bienveillant, dr√¥le et clair."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=800
    )
    return response