# backend/models/rag/generate_question.py

from typing import List, Tuple
from langchain.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEndpoint
from backend.models.rag.query_rag import answer_question


# === Configuration LLM
llm = HuggingFaceEndpoint(
    repo_id="tiiuae/falcon-7b-instruct",
    temperature=0.4,
    max_new_tokens=256,
    task="text-generation",
    huggingfacehub_api_token="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"
)

# === Prompt Template pour g√©n√©rer la prochaine question
prompt_template = PromptTemplate.from_template("""
Tu es un conseiller en orientation scolaire et professionnelle.
Tu dois poser une s√©rie de 7 questions personnalis√©es pour aider un jeune √† d√©couvrir les secteurs d'activit√©s et les m√©tiers qui pourraient lui correspondre.

Voici l'historique des questions d√©j√† pos√©es et des r√©ponses fournies :
{history}

Propose maintenant une nouvelle question simple, claire et ferm√©e (avec des choix). Ne pose pas une question d√©j√† pos√©e.
Ta r√©ponse doit √™tre au format :
Question: <texte de la question>
Choix: <choix1>, <choix2>, <choix3>, <choix4>
""")


def generate_next_question(history_pairs: list[tuple[str, str]]) -> str:
    # Reformatage de l'historique
    history_text = "\n".join([f"Q: {q}\nR: {r}" for q, r in history_pairs]) or "(aucune question pos√©e)"

    prompt = f"""
Tu joues le r√¥le d‚Äôun conseiller d‚Äôorientation expert. Tu vas poser 7 questions √† un utilisateur pour cerner son profil.

Voici les √©changes pr√©c√©dents :
{history_text}

Pose-moi une nouvelle question (courte, claire, adapt√©e √† un jeune) pour continuer √† mieux cerner ma personnalit√© et mes pr√©f√©rences professionnelles.

R√©ponds uniquement par la question, sans explication ni commentaire.
""".strip()

    # Appel au LLM
    response = answer_question(prompt)

    # Debug
    print("üß† R√©ponse du LLM pour la question suivante :")
    print(response)

    # Nettoyage de la r√©ponse : on garde la premi√®re ligne non vide
    lines = [line.strip() for line in response.split("\n") if line.strip()]
    if not lines:
        raise ValueError("Format de la r√©ponse du LLM incorrect.")
    
    return lines[0]

def generate_final_recommendation(history_pairs: List[Tuple[str, str]]) -> str:
    history_text = "\n".join([f"Q: {q}\nR: {r}" for q, r in history_pairs])
    prompt = f"""
Tu es un conseiller d‚Äôorientation scolaire et professionnelle. Voici le profil d‚Äôun utilisateur bas√© sur ses r√©ponses √† 7 questions :

{history_text}

√Ä partir de ces r√©ponses, propose 3 secteurs d‚Äôactivit√© dans lesquels il pourrait s‚Äô√©panouir.
Pour chaque secteur, donne 2 m√©tiers pertinents (un tr√®s connu, un moins connu).
Explique bri√®vement pourquoi ces suggestions sont adapt√©es √† la personne.
Donne une r√©ponse claire, bienveillante, professionnelle et directement exploitable.
"""

    response = llm.invoke(prompt)
    return response.strip()